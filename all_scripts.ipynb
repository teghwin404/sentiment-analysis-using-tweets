{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6169fc6e",
   "metadata": {},
   "source": [
    "## Twitter Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that the snscrape can't collect RETWEETS\n",
    "# Avoid high frequency requests:\n",
    "#  - For ONE keywords/users, you collect 100K tweets                     -> ONE request, it's fine\n",
    "#  - For 1000 keywords/users, you collect 1 tweet from each keyword/user -> 1K requests, may cause IP blocking\n",
    "\n",
    "# Please make sure the snscrape has been installed in the Python environment\n",
    "# If you are using PythonAnywhere\n",
    "#   Step 1: Open $Bash on your PythonAnywhere Dashboard\n",
    "#   Step 2: Enter the following command: pip3.9 install --user git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "# The Python version has to be greater than 3.8\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "key_word = \"biodegradable OR carbon OR climate OR climateaction OR climatechange OR climatecrisis OR earthday OR ecologic OR environment OR environmental OR nature OR natureforall OR organic OR paper OR paperbag OR paperbased OR paperless OR planet OR planetary OR plastic OR preserve OR recyclable OR recycle OR recycling OR reusable OR reusing OR sustainability OR sustainable OR unsustainable AND shein\" \n",
    "\n",
    "# used for shein since it is in the keyword\n",
    "\n",
    "user_name = \"@SHEIN_Official\"   # Declare a user name used to search tweets -> Tweet search by user\n",
    "from_date = \"2018-11-01\"      # Declare a start date\n",
    "end_date = '2022-11-01'       # Declare a end date\n",
    "count =30000                   # The maximum number of tweets\n",
    "\n",
    "tweets_list_keyword = [] # A list used to store the returned results for keyword search\n",
    "tweets_list_user = []    # A list used to store the retuned results for user search\n",
    "\n",
    "#### Scraping tweets from a specific keyword ####\n",
    "command_keyword = key_word+' since:'+from_date+' until:'+end_date\n",
    "print(\"Scraping data for keyword:\",key_word)\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(command_keyword).get_items()):\n",
    "    # For other available attributes: https://github.com/JustAnotherArchivist/snscrape/issues/115\n",
    "    tweets_list_keyword.append([tweet.date,tweet.id, tweet.rawContent, tweet.user.username, tweet.url,tweet.lang])\n",
    "    if i>count:\n",
    "        break;\n",
    "\n",
    "# Create a dataframe from the tweets list above\n",
    "tweets_df_keyword = pd.DataFrame(tweets_list_keyword, columns=['Datetime','Tweet Id', 'Text', 'Username', 'url','language'])\n",
    "tweets_df_keyword['Datetime'] = tweets_df_keyword['Datetime'].astype(str).str[:-6]\n",
    "tweets_df_keyword.to_csv(\"tweets_keywords_shein.csv\",index=False) # Export to a csv file\n",
    "tweets_df_keyword.to_excel(\"tweets_keywords_shein.xlsx\",index=False) # Uncomment this line if you prefer an Excel file\n",
    "print(\"Scraped data have been exported to the csv file\")\n",
    "\n",
    "\n",
    "Scraping tweets from a specific userâ€™s account\n",
    "command_user = 'from:'+user_name+' since:'+from_date+' until:'+end_date\n",
    "print(\"Scraping data for user:\",user_name) # we updated the user_name with the brand name\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(command_user).get_items()):\n",
    "    tweets_list_user.append([tweet.date,tweet.id, tweet.rawContent, tweet.user.username, tweet.url])\n",
    "    if i > count:\n",
    "        break;\n",
    "# Create a dataframe from the tweets list above\n",
    "tweets_df_user = pd.DataFrame(tweets_list_user, columns=['Datetime','Tweet Id', 'Text', 'Username', 'url'])\n",
    "tweets_df_user.to_csv(\"tweets_users_patagonia.csv\",index=False) # Export to a csv file\n",
    "tweets_df_user['Datetime'] = tweets_df_user['Datetime'].astype(str).str[:-6]\n",
    "tweets_df_user.to_excel(\"tweets_users_patagonia.xlsx\",index=False) # Uncomment this line if you prefer an Excel file\n",
    "print(\"Scraped data have been exported to the csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c24fe",
   "metadata": {},
   "source": [
    "## Word Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "inter1 = []\n",
    "sentences_all = []\n",
    "sentences_clean = []\n",
    "sentences_unpun = []\n",
    "\n",
    "dictionary1 = {}\n",
    "d2_dict = defaultdict(dict)\n",
    "\n",
    "with open('tweets_users_patagonia.csv') as f:\n",
    "    rows = csv.reader(f, delimiter = ',')\n",
    "    for row in rows:\n",
    "        inter1.append(row[2])\n",
    "\n",
    "# Split the row into different sentences\n",
    "for row in inter1:\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row)\n",
    "    for s in sentences:\n",
    "        in1 = ''.join(s)\n",
    "        out = re.sub('[%s]' % re.escape(string.punctuation), '', in1.lower())\n",
    "        sentences_all.append(out)\n",
    "\n",
    "# Remove stop words from sentence\n",
    "for sentence in sentences_all:\n",
    "    s = []\n",
    "    for i in sentence.split():\n",
    "        if i not in stop and i.isdigit() is False:\n",
    "            s.append(i)\n",
    "    sentences_clean.append(s)\n",
    "\n",
    "\n",
    "# Add each words as key into a dictionary\n",
    "for sentence in sentences_clean:\n",
    "    #print sentence\n",
    "    for word in sentence:\n",
    "        dictionary1[word] = 0\n",
    "\n",
    "# Update the frequency dictionary table\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        dictionary1[word] = dictionary1[word] + 1\n",
    "\n",
    "# Add each pair of words as key into a dictionary 2\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        for word2 in sentence:\n",
    "            if(word != word2):\n",
    "                d2_dict[word][word2] = 0\n",
    "# Update the frequency dictionary table\n",
    "for sentence in sentences_clean:\n",
    "    for word in sentence:\n",
    "        for word2 in sentence:\n",
    "            if(word != word2):\n",
    "                d2_dict[word][word2] = d2_dict[word][word2] + 1\n",
    "                \n",
    "\n",
    "writer = csv.writer(open('word_freq.csv', 'w',newline=''))\n",
    "for key, value in dictionary1.items():\n",
    "    writer.writerow([key, value])\n",
    "\n",
    "writer = csv.writer(open('word_pair_freq.csv', 'w',newline=''))\n",
    "for key1, value1 in d2_dict.items():\n",
    "    for key2, value2 in d2_dict[key1].items():\n",
    "        writer.writerow([key1, key2, value2])\n",
    "\n",
    "print (\"Wrote to word_freq.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89909b70",
   "metadata": {},
   "source": [
    "## Find and Replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "\n",
    "# Define the input file name\n",
    "filename = \"C:/Users/teghw/Desktop/Fall 2022/INSY 448 - Text and Social Media Analytics/final project/scraping from companies/zara/tweets_users_zara.csv\"\n",
    "# Define the output file name\n",
    "output = \"C:/Users/teghw/Desktop/Fall 2022/INSY 448 - Text and Social Media Analytics/final project/find and replace/by company/zara_company_new.csv\"\n",
    "# The list used to stored the replaced csv files \n",
    "\n",
    "output_list=[]\n",
    "with open(filename, 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        #This item is the forum post (3rd elements of each row)\n",
    "        with open(\"C:/Users/teghw/Desktop/Fall 2022/INSY 448 - Text and Social Media Analytics/final project/find and replace/keys.csv\", 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter=',')\n",
    "            for row2 in read:\n",
    "                #Find and Replace in Bruteforce way\n",
    "                row[2] = row[2].lower().replace(\" \" + row2[1].lower() + \" \",row2[0].lower())\n",
    "        output_list.append(row)\n",
    "\n",
    "# Write the values in output list to the output file\n",
    "with open(output, 'w', newline='') as output:\n",
    "    writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(output_list)\n",
    "print (\"Wrote to x_company_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdae15e",
   "metadata": {},
   "source": [
    "## Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3.0\n",
    "# if required install twython: pip install twython\n",
    "# the input filename is limit_post.csv in line 34, change it as needed\n",
    "# the output file is sentiment_data.xlsx, change it when running the script multiple times\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "def get_sentiment(rating_data):\n",
    "    \"\"\"\n",
    "    https: // github.com / cjhutto / vaderSentiment\n",
    "    \"\"\"\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    rating_data['sent_neg'] = -10\n",
    "    rating_data['sent_neu'] = -10\n",
    "    rating_data['sent_pos'] = -10\n",
    "    rating_data['sent_compound'] = -10\n",
    "    for i in range(len(rating_data)):\n",
    "        sentence = rating_data['Sentences'][i]\n",
    "        ss = sid.polarity_scores(sentence.encode('ascii', 'ignore').decode(\"ascii\"))\n",
    "        #print (ss['neg'])\n",
    "        rating_data.iloc[i, 1] = float(ss['neg'])\n",
    "        #print (rating_data.iloc[i, 1])\n",
    "        rating_data.iloc[i, 2] = ss['neu']\n",
    "        rating_data.iloc[i, 3] = ss['pos']\n",
    "        rating_data.iloc[i, 4] = ss['compound']\n",
    "    return rating_data\n",
    "\n",
    "rating_data = pd.read_csv(\"limit_post.csv\", encoding = 'latin1')\n",
    "rating_data = rating_data.rename(columns={rating_data.columns[0]: \"Sentences\" })\n",
    "sentiment_data = get_sentiment(rating_data)\n",
    "sentiment_data.to_excel(\"sentiment_data.xlsx\", index = False)\n",
    "print (\" Written to sentiment_data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
